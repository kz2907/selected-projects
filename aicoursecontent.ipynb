{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zDXam8A4uXSM",
        "outputId": "6afe8fcf-4673-4d80-9b9a-e1d99e05f84b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.15.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading pymongo-4.15.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/331.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.8.0 pymongo-4.15.5\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.16.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Downloading qdrant_client-1.16.1-py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, qdrant-client\n",
            "Successfully installed portalocker-3.2.0 qdrant-client-1.16.1\n",
            "Collecting fastembed\n",
            "  Downloading fastembed-0.7.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.20 in /usr/local/lib/python3.12/dist-packages (from fastembed) (0.36.0)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from fastembed) (2.0.2)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pillow<12.0,>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from fastembed) (11.3.0)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed)\n",
            "  Downloading py_rust_stemmers-0.1.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.12/dist-packages (from fastembed) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from fastembed) (0.22.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.12/dist-packages (from fastembed) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.20->fastembed) (1.2.0)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.14.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.31->fastembed) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.31->fastembed) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.31->fastembed) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.31->fastembed) (2025.11.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
            "Downloading fastembed-0.7.4-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_rust_stemmers-0.1.5-cp312-cp312-manylinux_2_28_x86_64.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py-rust-stemmers, mmh3, loguru, humanfriendly, coloredlogs, onnxruntime, fastembed\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.7.4 humanfriendly-10.0 loguru-0.7.3 mmh3-5.2.0 onnxruntime-1.23.2 py-rust-stemmers-0.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo\n",
        "!pip install qdrant-client\n",
        "!pip install fastembed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s2O5AF56uftS",
        "outputId": "41b2c60f-4e5e-4e62-8f94-ecbdfcf50a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MongoDB collection 'pages' cleared.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3573207016.py:63: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'crawled_at': datetime.utcnow(),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/transformers-intro.html\n",
            "  Added: https://pantelis.github.io/index.html\n",
            "  Added: https://pantelis.github.io/book/foundations/index.html\n",
            "  Added: https://pantelis.github.io/book/dnn/index.html\n",
            "  Added: https://pantelis.github.io/book/2d-perception/index.html\n",
            "  Added: https://pantelis.github.io/book/kinematics/index.html\n",
            "  Added: https://pantelis.github.io/book/state-estimation/index.html\n",
            "  Added: https://pantelis.github.io/book/llm/index.html\n",
            "  Added: https://pantelis.github.io/book/multimodal/index.html\n",
            "  Added: https://pantelis.github.io/book/task-planning/index.html\n",
            "  Added: https://pantelis.github.io/book/global-planning/index.html\n",
            "  Added: https://pantelis.github.io/book/local-planning/index.html\n",
            "  Added: https://pantelis.github.io/book/mdp/index.html\n",
            "  Added: https://pantelis.github.io/book/rl/index.html\n",
            "  Added: https://pantelis.github.io/book/vla/index.html\n",
            "  Added: https://pantelis.github.io/courses/ai/index.html\n",
            "  Added: https://pantelis.github.io/courses/robotics/index.html\n",
            "  Added: https://pantelis.github.io/courses/cv/index.html\n",
            "  Added: https://pantelis.github.io/data-mining/\n",
            "  Added: https://ai-robotics.one-aegean-ai.workers.dev/\n",
            "  Added: https://pantelis.github.io/about.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/nlp-pipelines/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/tokenization/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/word2vec_from_scratch.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nlp-introduction/word2vec/word2vec_tensorflow_tutorial.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/rnn/introduction/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/rnn/simple-rnn/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/rnn/time_series_using_simple_rnn_lstm.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/rnn/lstm/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/rnn-language-model/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/simple-rnn-language-model/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nmt/nmt-intro/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nmt/rnn-nmt-workshop/lstm_seq2seq.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nmt/nmt-metrics/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/nmt/rnn-nmt-attention/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/mlp.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/inference/index.html\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/nlp/transformers/transformers-intro.md\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/nlp/transformers/transformers-intro.md\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/issues/new\n",
            "  Added: https://github.com/lucidrains/x-transformers\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/singlehead-self-attention.html\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/nlp/transformers/singlehead-self-attention.md\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/nlp/transformers/singlehead-self-attention.md\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/multihead-self-attention.html\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/nlp/transformers/multihead-self-attention.md\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/nlp/transformers/multihead-self-attention.md\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/positional_embeddings.html\n",
            "  Added: https://github.dev/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/nlp/transformers/positional_embeddings.ipynb\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/nlp/transformers/positional_embeddings.ipynb\n",
            "Error crawling https://pantelis.github.io/aiml-common/lectures/nlp/transformers/mlp.htmlhttps://pantelis.github.io/aiml-common/lectures/VLM/clip/index.html#clip-architecture: 404 Client Error: Not Found for url: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/mlp.htmlhttps://pantelis.github.io/aiml-common/lectures/VLM/clip/index.html#clip-architecture\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3573207016.py:104: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  'crawled_at': datetime.utcnow(),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/VLM/clip/paper.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/optimization/batch-normalization/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/optimization/layer-normalization/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vision-transformers/paper.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vision-transformers/vision_transformer_tutorial.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/VLM/clip/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/VLM/blip-2/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/VLM/blip-2/paper.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/VLM/llava/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/VLM/llava/paper.html\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/VLM/clip/paper.qmd\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/VLM/clip/paper.qmd\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/vae/elbo-optimization/elbo_optimization_torch.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/generative-modeling/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/generative-modeling/em-algorithm/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/generative-modeling/em-gaussian-mixture/em_example_mog.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vae/introduction/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vae/vae-architecture/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vae/elbo-optimization/vae.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/stable-diffusion/harvard-tutorial/MLFS_StableDiffusion_Playground.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/stable-diffusion/harvard-tutorial/MLFS_Build_Your_StableDiffusion_in_a_Notebook.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/stable-diffusion/harvard-tutorial/Diffusion_Model_with_Cross_Attention_(student).html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/stable-diffusion/harvard-tutorial/index.html\n",
            "  Added: https://github.dev/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/vae/elbo-optimization/elbo_optimization_torch.ipynb\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/vae/elbo-optimization/elbo_optimization_torch.ipynb\n",
            "  Added: https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/vae.py\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/vae/introduction/\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/vae/introduction/index.qmd\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/vae/introduction/index.qmd\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/vae/introduction/mnist-manifold-preview.html\n",
            "  Saved to MongoDB: https://pantelis.github.io/aiml-common/lectures/vae/vae-architecture/\n",
            "  Added: https://arxiv.org/pdf/1906.02691.pdf\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/aiml-common/lectures/vae/vae-architecture/index.qmd\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/aiml-common/lectures/vae/vae-architecture/index.qmd\n",
            "  Saved to MongoDB: https://pantelis.github.io/index.html\n",
            "  Added: https://jupyter.org\n",
            "  Added: https://pantelis.github.io/courses.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/learning-problem/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/scene-understanding-intro/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/pytorch/detectron_tutorial.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/pytorch/maskrcnn_detectron.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/pytorch/maskrcnn_torchvision_inference.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/tf/demo.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/tf/inspect_data.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/tf/inspect_model.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/maskrcnn/tf/inspect_weights.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/scene-understanding/semantic-segmentation/unet/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/finetuning/patents/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/change-me.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/cnn-language-model/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/language-models/lstm-language-model/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/annotated_transformer.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/attention_scaling_explained.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/convert_adyen_tgi_article.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/nlp/transformers/scaling.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/logical-reasoning/automated-reasoning/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/logical-reasoning/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/logical-reasoning/logical-agents/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/logical-reasoning/logical-inference/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/logical-reasoning/propositional-logic/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/pddl/blocksworld/fixme.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/pddl/blocksworld/up_blocksworld_demo.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/pddl/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/pddl/logistics/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/planning/task-planning/pddl/manufacturing/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/bellman-expectation-backup/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/bellman-optimality-backup/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/policy-iteration/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/policy-iteration/policy-iteration-gridworld.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/policy-iteration/policy_iteration.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/value-iteration/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/value-iteration/recycling_robot_qstar_value_iteration_fixed.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/value-iteration/value-iteration-gridworld.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/dynamic-programming-algorithms/value-vs-policy-iteration.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-intro/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-intro/mdp_intro.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/car-rental/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/cleaning-robot/deterministic_mdp.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/cleaning-robot/stochastic_mdp.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/fomdp-example/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/pomdp-example/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/recycling-robot/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/mdp-workshop/yield-management-capacity-control/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/policy-evaluation/aima.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/policy-evaluation/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/mdp/policy-improvement/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/model-free-control/generalized-policy-iteration/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/model-free-control/greedy-monte-carlo/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/model-free-control/sarsa/gridworld/sarsa_gridworld.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/model-free-control/sarsa/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/policy-based-algorithms/policy-gradient/policy-gradient-pong-game.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/policy-based-algorithms/reinforce/index.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/prediction/monte-carlo.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/prediction/prediction-example.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/prediction/td-vs-mc-mrp.html\n",
            "  Added: https://pantelis.github.io/aiml-common/lectures/reinforcement-learning/prediction/temporal-difference.html\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/edit/master/index.qmd\n",
            "  Added: https://github.com/pantelis/engineering-ai-agents/blob/master/index.qmd\n",
            "\n",
            "Crawl finished. Visited 10 unique URLs.\n",
            "Stored 10 documents in MongoDB\n"
          ]
        }
      ],
      "source": [
        "#Data Pipeline\n",
        "from collections import deque\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# MongoDB setup\n",
        "client = MongoClient('mongodb+srv://kz2907_db_user:hAJWM5s3VwhPX0Q5@cluster0.yrw4ow1.mongodb.net/?appName=Cluster0')  # Change later\n",
        "db = client['web_crawler']  # Database name\n",
        "collection = db['pages']  # Collection name\n",
        "\n",
        "collection.delete_many({})\n",
        "print(\"MongoDB collection 'pages' cleared.\")\n",
        "\n",
        "#start_url = 'https://pantelis.github.io/courses/ai/in-person.html'\n",
        "base_domain = 'pantelis.github.io'\n",
        "\n",
        "manual_urls = [\n",
        "    'https://pantelis.github.io/aiml-common/lectures/nlp/transformers/transformers-intro.html',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/nlp/transformers/singlehead-self-attention.html' ,\n",
        "    'https://pantelis.github.io/aiml-common/lectures/nlp/transformers/multihead-self-attention.html',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/nlp/transformers/positional_embeddings.html',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/nlp/transformers/mlp.html'\n",
        "    'https://pantelis.github.io/aiml-common/lectures/VLM/clip/index.html#clip-architecture',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/VLM/clip/paper.html',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/vae/elbo-optimization/elbo_optimization_torch.html',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/vae/introduction/',\n",
        "    'https://pantelis.github.io/aiml-common/lectures/vae/vae-architecture/'\n",
        "]\n",
        "\n",
        "to_visit = deque(manual_urls)\n",
        "visited = set()\n",
        "crawled_count = 0\n",
        "max_pages_to_crawl = 10\n",
        "\n",
        "while to_visit and crawled_count < max_pages_to_crawl:\n",
        "    current_url = to_visit.popleft()\n",
        "\n",
        "    if current_url in visited:\n",
        "        continue\n",
        "\n",
        "    visited.add(current_url)\n",
        "    crawled_count += 1\n",
        "\n",
        "    current_domain = urlparse(current_url).netloc #extracts hostname of the website being crawled\n",
        "\n",
        "    try:\n",
        "        response = requests.get(current_url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract page data\n",
        "        page_data = {\n",
        "            'url': current_url,\n",
        "            'domain': current_domain,\n",
        "            'title': soup.title.string if soup.title else None,\n",
        "            'text': soup.get_text(separator=' ', strip=True),\n",
        "            'html': response.text,\n",
        "            'status_code': response.status_code,\n",
        "            'crawled_at': datetime.utcnow(),\n",
        "            'is_base_domain': current_domain == base_domain\n",
        "        }\n",
        "\n",
        "        # Store in MongoDB\n",
        "        collection.insert_one(page_data)\n",
        "        print(f\"  Saved to MongoDB: {current_url}\")\n",
        "\n",
        "        # Only extract links if we're on a page from the base domain\n",
        "        if current_domain == base_domain:\n",
        "            links_found = []\n",
        "            for link in soup.find_all('a'):\n",
        "                href = link.get('href')\n",
        "                if href:\n",
        "                    # Resolve relative URLs using the current page as base\n",
        "                    absolute_url = urljoin(current_url, href)\n",
        "\n",
        "                    # Remove URL fragments (#section)\n",
        "                    absolute_url = absolute_url.split('#')[0]\n",
        "\n",
        "                    parsed_url = urlparse(absolute_url)\n",
        "\n",
        "                    if parsed_url.scheme in ['http', 'https'] and absolute_url not in visited and absolute_url not in to_visit:\n",
        "                        to_visit.append(absolute_url)\n",
        "                        links_found.append(absolute_url)\n",
        "                        print(f\"  Added: {absolute_url}\")\n",
        "\n",
        "            # Update the document with extracted links\n",
        "            collection.update_one(\n",
        "                {'url': current_url},\n",
        "                {'$set': {'links': links_found}}\n",
        "            )\n",
        "        else:\n",
        "            print(f\"  External site - not extracting links\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error crawling {current_url}: {e}\")\n",
        "        # Store error in MongoDB\n",
        "        collection.insert_one({\n",
        "            'url': current_url,\n",
        "            'error': str(e),\n",
        "            'crawled_at': datetime.utcnow(),\n",
        "            'status': 'failed'\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred for {current_url}: {e}\")\n",
        "\n",
        "print(f\"\\nCrawl finished. Visited {len(visited)} unique URLs.\")\n",
        "print(f\"Stored {collection.count_documents({})} documents in MongoDB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ex16cHbeujWx"
      },
      "outputs": [],
      "source": [
        "#Load data from MongoDB\n",
        "from collections import deque\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for doc in collection.find():\n",
        "    data.append({\n",
        "        'title': doc.get('title'),\n",
        "        'text': doc.get('text'),\n",
        "        'url': doc.get('url')\n",
        "    })\n",
        "\n",
        "df_pages = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eEpumX3Uumo0",
        "outputId": "57171136-61c0-4b8a-e6ef-05053fa9667a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "676a39fa07394847953afc573156a51b",
            "5061e05fe3494fa28a311784c046b6f4",
            "150db002ce0346e5a5250d66e85168d0",
            "45cb81df136248d8ab5781b96a3a2ddc",
            "4e81fe7ed124440c8244d5f004a0b4b8",
            "9603270edfa14672a7a85982ef90d7fc",
            "6405efab3ece4c2cab61bd6fd339cf96",
            "89bc659efe6e4be7aa774dc1f1723938",
            "00783cfeaf95466ca275e1fea4699f17",
            "20dea9da3b5b486da1ee2467a10991eb",
            "a0902d5c8859463fb48417a2d05c718c",
            "c32e92d2f017428a9dcfaa9f312d5ae0",
            "6ff13ee14e1c4590bc15f47521c12c73",
            "89f86d08d8f946549b0d088e3cfac4a9",
            "6043ca2002bb4486a59184cb0f85d71f",
            "88f636eb0b0b45458cb5d9c82399298c",
            "82cdf360ae6246b1833b811738bfe86f",
            "b849363f7ae644dba079bd964fef9a59",
            "7827e0efd439469cb52e152d85bca5ff",
            "6fc0112ad2a14ad1aceffb766fe8ec2a",
            "2189a9deaa8f4e71bd307da8d3efc15d",
            "f1f74637c78b4ac98b56357b2f9c4014",
            "05bec891d1534221aca3b97ef8215795",
            "2dbd8e6801fe4e6fa3bcf51b5d845c8e",
            "460cf50e83bc4ae88977f49032214924",
            "6757d06fbcbf4e8ab5970db6222b5e7d",
            "fb280013e9974d32ae5ed9ddd08ac5dc",
            "b303e069ddeb419d867dd1c3d27078dc",
            "bee65c40f03349b382660604db9c2ad2",
            "1a16952230c0438d9c7666f57a914ff2",
            "44cb6cdc851f466f9823a15e56cc6806",
            "ba9b276006794bb1b1bd5a01ab634914",
            "e54b004797734a2ca04e7c7de7aee8f5",
            "bb7eead9f1614d00b245c796c5db563e",
            "d7eff50712254d7c93f6acf5b983ca44",
            "d198009ecde04616be061faee16deddd",
            "cdbbe95be4114e8e88e167b92b7d9a60",
            "7bda3bff27de476cab22465de68b480e",
            "bf4019e4042a46099c79f0d8d50ada9a",
            "5135d54ddd414be89460e3701219f07d",
            "5a9899567c6844cd9c4111853ca7e2bb",
            "fc48cefdd5094acd986d104be1f0d03c",
            "de752ff523f84387a2381836efd9da76",
            "66df6064a06e4bd5a09a779094cc40af",
            "d1a23d19bb3f4c809f0da53d2957e8b8",
            "4473ac97c9174d81a489d78811392ba9",
            "583f7316b6844c15b234d500e446a6e6",
            "0faae67b7e8e472eb8acaa42f2691a36",
            "1d9eb65d9237411ebaa1cbbe95847204",
            "db40ef6e180e459d8a5fb5e115377474",
            "6c443b214de1465dbc8c0f021b1d5f4a",
            "c634fd2729264820bc0e951b2092acd1",
            "12ead9a8e7f84f83932872bd938bf85b",
            "54c419ef4ff344f7bdb57ab2c76e1584",
            "27fbd6c55071485ab6e323675d9dfa4e",
            "6afda33939d042e2849c0f3f707a0f8f",
            "b80b2e21158e4285bed7cda3f771bc97",
            "9adc5ca9cbee49399cf5a082dab4e3aa",
            "cf019ab3f07f4e0ab3cd35badb2702ff",
            "42290c15f35d4998be0318db8206e912",
            "4615b08aacaf4246a543e9606369f5cb",
            "76abf679edf64d60a8367182d42afb15",
            "0c194904a1684e5cb7514545e9882174",
            "a08708effac94fc2bb31ca524858f3fc",
            "f39c5a0696d84a959f56da48d98ea040",
            "1d2b8b25397b451cbf13ff66ae1b6d73",
            "89427baf9c3b408bafa4e8ab52acb053",
            "eee63aa698db489c9ac9398a34f994e5",
            "16a7eae95a4242548f8acaf23f746a00",
            "1eadc7282faa46eb999159b0730a396d",
            "d486da9a61ca44c3a7da2243f7b7880c",
            "15ce6906d1af4069b4659bd54f04e3dc",
            "c2b3c6c290a94495be30a2b4988630f3",
            "ca45ecfb9eef41f381943c0e147bfa52",
            "3859df6e884f4f0cba9450c3df77da52",
            "2c449418d97a4367ae04fd26e19754eb",
            "80e4e0b563e54fadb9ec367cca5f7e47",
            "e5855f4aca8746d6b486ee2f3e7d8be1",
            "ec0f90a5654a42b6ad4bf9bdec06ea7d",
            "e36745e3863b40a8a227ec8c46743426",
            "ded09989426b429cb8e996d173216ddd",
            "c4d681f6fbc348e59c0142009bd6e27b",
            "3f5615067de043c69fba655efafec0fd",
            "4583be6c0f134c469f40c2688ec66cd8",
            "00f1cc7c5ca94c5fa07e2db370071480",
            "4c3917c7ecfc4ffa83893af27fa828c7",
            "a7b0442231844d828b5a0675f27ba0e2",
            "4852513932c34f9181ad0c1433e62ac4",
            "51f6b43732ac4d86ad364e629f497aff",
            "749bd4543164408695bbc31c41208e4c",
            "55c13dbbf8294d1d841b1743875fcf47",
            "3a60d58bfae5423eace1066ea44a9e8e",
            "7cdc7366318649ddaff5cb97f08a4e71",
            "668184491be64307a100bfd5f9796204",
            "1cb3a1254e464444a480e81d3165c20d",
            "b9524db4e73d45a89bfa6ccb5ad88d6e",
            "95a112c8e4164433ba397668e129be4e",
            "20b85871c8db41278ec9391e5da87e99",
            "51888e60e6ab43fe97cf9d2ccb00797e",
            "7d49cf9ac503444f9628c26954f1de63",
            "eb10cd58003b4d618f563de3f72cb571",
            "2f816d271255466a98fc98bfe1516e62",
            "8ff357e5483a41969229c2a4f67dfc4f",
            "d8c673b36d98470e8f78ca3ff7f72d5a",
            "692602521328445cbf1dd2ca77d9e428",
            "b99396e606074638921b8f08df7c348e",
            "7e3341e29eab4e57afec44f58b35e461",
            "f571c099396a48d2b3406bf89a46d3fd",
            "a005af8609fd4c57a52142e87c099b32",
            "a52db0f6fa764b738d2524c435e3e816",
            "e8fa5d5778794fb0ab8ed33822f8a944",
            "4a4546a998094cf49ac08fe25079569a",
            "3d037d7489eb4d99b6711ec0e9f99b04",
            "b55d6d286ef045028967bc084312469f",
            "ddb68217b0a74afbac76b0796349267f",
            "20228a78f36c4579bc70f20b86f7d3dc",
            "cc0bc99239324814b2eace3b61b692fd",
            "b55230443fcb446f9371f2bc38f763f5",
            "09db23de268d4dfa9383bf788b9a42dc",
            "ae3e2fb045874b149c62f3d79bf96a8f",
            "9948a19c6f014eeb83fec60efc7d6441"
          ]
        },
        "collapsed": true,
        "id": "1w-lmBzpuvtA",
        "outputId": "cfacaa84-429a-4c09-8721-cbd657cc3067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 103 chunks from 10 documents\n",
            "First chunk: transformers-intro – Engineering AI Agents Engineering AI Agents BOOK Foundations Training Deep Netw...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "676a39fa07394847953afc573156a51b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c32e92d2f017428a9dcfaa9f312d5ae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05bec891d1534221aca3b97ef8215795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb7eead9f1614d00b245c796c5db563e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1a23d19bb3f4c809f0da53d2957e8b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6afda33939d042e2849c0f3f707a0f8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89427baf9c3b408bafa4e8ab52acb053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5855f4aca8746d6b486ee2f3e7d8be1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51f6b43732ac4d86ad364e629f497aff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d49cf9ac503444f9628c26954f1de63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8fa5d5778794fb0ab8ed33822f8a944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings for 103 chunks.\n",
            "Shape of first embedding: 384\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def count_tokens(text, model_name=\"cl100k_base\"): \n",
        "    \"\"\"Count tokens in text\"\"\"\n",
        "    encoding = tiktoken.get_encoding(model_name) \n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def chunk_text(text, chunk_size=256, chunk_overlap=50):\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    if not isinstance(text, str) or len(text) < chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        if count_tokens(current_chunk + \" \" + sentence) > chunk_size and current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            overlap_sentences = current_chunk.split()[-chunk_overlap//3:]\n",
        "            current_chunk = \" \".join(overlap_sentences) + \" \" + sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_rag_chunks(df, text_column='text'):\n",
        "    \"\"\"Create RAG chunks from dataframe\"\"\"\n",
        "    chunks = []\n",
        "    metadata = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text_chunks = chunk_text(row[text_column])\n",
        "\n",
        "        for chunk_idx, chunk in enumerate(text_chunks):\n",
        "            chunks.append(chunk)\n",
        "            metadata.append({\n",
        "                'document_id': idx,\n",
        "                'chunk_id': chunk_idx,\n",
        "                'title': row.get('title', ''),\n",
        "                'url': row.get('url', ''),\n",
        "                'total_chunks': len(text_chunks)\n",
        "            })\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks from {len(df)} documents\")\n",
        "    return chunks, metadata\n",
        "\n",
        "# Create chunks\n",
        "chunks, metadata = create_rag_chunks(df_pages)\n",
        "print(f\"First chunk: {chunks[0][:100]}...\")\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
        "chunk_embeddings = model.encode(chunks)\n",
        "dim = len(chunk_embeddings[0])\n",
        "print(f\"Generated embeddings for {len(chunk_embeddings)} chunks.\")\n",
        "print(f\"Shape of first embedding: {dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zLJgm8i5uyrW",
        "outputId": "8f901342-4de3-4c1f-9275-e0b1060311da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T1CFy63Cvhg_",
        "outputId": "ea06bd51-f81a-4192-aca3-1f9f2b410bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "/usr/local/bin/ollama\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "!which ollama\n",
        "\n",
        "# Terminate any existing Ollama server process to ensure a clean restart\n",
        "!pkill -9 ollama\n",
        "\n",
        "!nohup ollama serve > /tmp/ollama.log 2>&1 &\n",
        "!ollama pull qwen:7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GmeRUGiku4jl"
      },
      "outputs": [],
      "source": [
        "#LLM Query\n",
        "import requests\n",
        "\n",
        "def ollama_llm(prompt, model=\"qwen3:0.6b\"):\n",
        "    \"\"\"Simple Ollama query - no timeouts, no fallbacks\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False\n",
        "            }\n",
        "        )\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "        if \"response\" in response_json:\n",
        "            return response_json[\"response\"]\n",
        "        else:\n",
        "            print(f\"Ollama response did not contain 'response' key. Full response: {response_json}\")\n",
        "            return \"\"\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        print(f\"Could not connect to Ollama server. Please ensure Ollama is running and the model is pulled. Error: {e}\")\n",
        "        return \"\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during Ollama API request: {e}\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred in ollama_llm: {e}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2GeU-xXpu5aO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def ask_llm(prompt, json_format=None):\n",
        "    \"\"\"Simple LLM query with JSON parsing\"\"\"\n",
        "    response = ollama_llm(prompt)\n",
        "    clean_response = response.strip()\n",
        "\n",
        "    # Extract JSON from code blocks\n",
        "    if '```json' in clean_response:\n",
        "        clean_response = clean_response.split('```json')[1].split('```')[0]\n",
        "    elif '```' in clean_response:\n",
        "        clean_response = clean_response.split('```')[1].split('```')[0]\n",
        "\n",
        "    # Return raw string if no JSON format requested\n",
        "    if not json_format:\n",
        "        return clean_response\n",
        "\n",
        "    # Try to parse JSON\n",
        "    try:\n",
        "        return json.loads(clean_response)\n",
        "    except:\n",
        "        # If parsing fails, return empty default\n",
        "        if json_format == \"list\":\n",
        "            return []\n",
        "        elif json_format == \"dict\":\n",
        "            return {}\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMk6wV2tvnSl",
        "outputId": "3f2021f0-c877-4331-c169-0341ab3c3258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 103 resource nodes\n",
            "\n",
            "Extracting concept candidates (smart filtering)...\n",
            "  Title candidate: Positional Embeddings\n",
            "  Title candidate: Positional Embeddings\n",
            "  Title candidate: Positional Embeddings\n",
            "  Title candidate: CLIP Paper\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: Variational Autoencoder from Scratch - Torch\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: MNIST Manifold\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "  Title candidate: VAE Architecture\n",
            "\n",
            "Found 5 candidates from titles/headers\n",
            "After manual addition: 15 candidates\n",
            "\n",
            "First 20 candidates:\n",
            "  1. Evidence Lower Bound\n",
            "  2. CLIP Paper\n",
            "  3. Self-Attention\n",
            "  4. VAE Architecture\n",
            "  5. Jensen's Inequality\n",
            "  6. Contrastive Learning\n",
            "  7. Attention Mechanism\n",
            "  8. Transformer\n",
            "  9. CLIP\n",
            "  10. Positional Embeddings\n",
            "  11. VAE\n",
            "  12. MNIST Manifold\n",
            "  13. ELBO\n",
            "  14. Variational Autoencoder\n",
            "  15. Variational Autoencoder from Scratch - Torch\n",
            "\n",
            "Processing batch: ['Evidence Lower Bound', 'CLIP Paper', 'Self-Attention', 'VAE Architecture', \"Jensen's Inequality\"]\n",
            " Concept: Evidence Lower Bound\n",
            " Concept: CLIP Paper\n",
            " Concept: Self-Attention\n",
            " Concept: VAE Architecture\n",
            " Concept: Jensen's Inequality\n",
            "\n",
            "Processing batch: ['Contrastive Learning', 'Attention Mechanism', 'Transformer', 'CLIP', 'Positional Embeddings']\n",
            " Concept: Contrastive Learning\n",
            " Concept: Attention Mechanism\n",
            " Concept: Transformer\n",
            " Concept: CLIP\n",
            " Concept: Positional Embeddings\n",
            "\n",
            "Processing batch: ['VAE', 'MNIST Manifold', 'ELBO', 'Variational Autoencoder', 'Variational Autoencoder from Scratch - Torch']\n",
            " Concept: VAE\n",
            " Concept: MNIST Manifold\n",
            " Concept: ELBO\n",
            " Concept: Variational Autoencoder\n",
            " Concept: Variational Autoencoder from Scratch - Torch\n",
            "\n",
            "Created 15 concept nodes\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "concept_nodes = []\n",
        "resource_nodes = []\n",
        "example_nodes = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    resource = {\n",
        "        'id': str(uuid.uuid4()),\n",
        "        'type': metadata[i].get('type', 'web'),\n",
        "        'url': metadata[i]['url'],\n",
        "        'title': metadata[i]['title'],\n",
        "        'span': chunk,\n",
        "        'timecodes': metadata[i].get('timecodes', [])\n",
        "    }\n",
        "    resource_nodes.append(resource)\n",
        "\n",
        "print(f\"Created {len(resource_nodes)} resource nodes\")\n",
        "\n",
        "potential_concepts = set()\n",
        "\n",
        "for resource in resource_nodes:\n",
        "    title = resource['title']\n",
        "    if title:\n",
        "        # Clean title\n",
        "        clean_title = title.strip()\n",
        "        # Remove common prefixes/suffixes\n",
        "        for prefix in ['- Engineering AI', '– Engineering AI', '| Engineering AI']:\n",
        "            if prefix in clean_title:\n",
        "                clean_title = clean_title.split(prefix)[0].strip()\n",
        "\n",
        "        if 2 <= len(clean_title.split()) <= 6:\n",
        "            potential_concepts.add(clean_title)\n",
        "            print(f\"  Title candidate: {clean_title}\")\n",
        "\n",
        "header_keywords = ['# ', '## ', '### ', '<h1>', '<h2>', '<h3>']\n",
        "\n",
        "for resource in resource_nodes:\n",
        "    content = resource['span']\n",
        "    if not content:\n",
        "        continue\n",
        "\n",
        "    lines = content.split('\\n')\n",
        "    for line in lines[:50]:  # Check first 50 lines\n",
        "        line = line.strip()\n",
        "\n",
        "        # Look for markdown headers\n",
        "        if line.startswith('# '):\n",
        "            header = line.replace('# ', '').strip()\n",
        "            if 2 <= len(header.split()) <= 5:\n",
        "                potential_concepts.add(header)\n",
        "\n",
        "        # Look for HTML headers (simplified)\n",
        "        elif '<h1>' in line or '<h2>' in line or '<h3>' in line:\n",
        "            # Extract text between tags\n",
        "            import re\n",
        "            headers = re.findall(r'<h[1-3]>(.*?)</h[1-3]>', line)\n",
        "            for header in headers:\n",
        "                if 2 <= len(header.split()) <= 5:\n",
        "                    potential_concepts.add(header.strip())\n",
        "\n",
        "print(f\"\\nFound {len(potential_concepts)} candidates from titles/headers\")\n",
        "\n",
        "#Manual Addition of main concepts\n",
        "MANUAL_CONCEPTS = [\n",
        "    \"Attention Mechanism\",\n",
        "    \"Transformer\",\n",
        "    \"Self-Attention\",\n",
        "    \"CLIP\",\n",
        "    \"Contrastive Learning\",\n",
        "    \"Variational Autoencoder\",\n",
        "    \"VAE\",\n",
        "    \"Jensen's Inequality\",\n",
        "    \"ELBO\",\n",
        "    \"Evidence Lower Bound\"\n",
        "]\n",
        "\n",
        "for concept in MANUAL_CONCEPTS:\n",
        "    potential_concepts.add(concept)\n",
        "\n",
        "print(f\"After manual addition: {len(potential_concepts)} candidates\")\n",
        "\n",
        "# Convert to list and remove duplicates/none\n",
        "all_titles = [t for t in list(potential_concepts) if t and len(t.strip()) > 0]\n",
        "\n",
        "print(f\"\\nFirst 20 candidates:\")\n",
        "for i, title in enumerate(all_titles[:20]):\n",
        "    print(f\"  {i+1}. {title}\")\n",
        "\n",
        "batch_size = 5\n",
        "seen_concepts = set()\n",
        "\n",
        "for i in range(0, len(all_titles), batch_size):\n",
        "    batch = all_titles[i:i + batch_size]\n",
        "    print(f\"\\nProcessing batch: {batch}\")\n",
        "\n",
        "    prompt = f\"\"\"Are these educational concepts? Return JSON array.\n",
        "\n",
        "Candidates: {batch}\n",
        "\n",
        "Return: [{{\"title\": \"exact name\", \"is_concept\": true/false}}]\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        results = ask_llm(prompt, json_format=\"list\")\n",
        "\n",
        "        if results:\n",
        "            for result in results:\n",
        "                if result.get('is_concept'):\n",
        "                    concept = {\n",
        "                        'id': str(uuid.uuid4()),\n",
        "                        'title': result['title'],\n",
        "                        'difficulty': result.get('difficulty', 'intermediate'),\n",
        "                        'aliases': result.get('aliases', []),\n",
        "                        'definitions': [result.get('definition', f\"Explanation of {result['title']}\")]\n",
        "                    }\n",
        "                    concept_nodes.append(concept)\n",
        "                    seen_concepts.add(result['title'])\n",
        "                    print(f\" Concept: {result['title']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "        # Add anyway for important ones\n",
        "        for title in batch:\n",
        "            if any(keyword in title.lower() for keyword in ['attention', 'transformer', 'clip', 'vae', 'jensen']):\n",
        "                concept = {\n",
        "                    'id': str(uuid.uuid4()),\n",
        "                    'title': title,\n",
        "                    'difficulty': 'intermediate',\n",
        "                    'aliases': [],\n",
        "                    'definitions': [f\"Explanation of {title}\"]\n",
        "                }\n",
        "                concept_nodes.append(concept)\n",
        "                print(f\" Manual add: {title}\")\n",
        "\n",
        "print(f\"\\nCreated {len(concept_nodes)} concept nodes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bura6-olvq9b",
        "outputId": "6f3c840a-a131-4e62-f5d3-83a06e431468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total nodes: 118\n"
          ]
        }
      ],
      "source": [
        "all_nodes = concept_nodes + resource_nodes + example_nodes\n",
        "print(f\"Total nodes: {len(all_nodes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hffg977kvx3S"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "edges = []\n",
        "\n",
        "#explains\n",
        "explains_edges = 0\n",
        "for resource in resource_nodes:\n",
        "    if resource['span'] is not None:  # Add check for None\n",
        "        resource_text = resource['span'].lower()\n",
        "        for concept in concept_nodes:\n",
        "            # Check if concept title is mentioned in resource\n",
        "            if concept['title'].lower() in resource_text:\n",
        "                edges.append({\n",
        "                    'source': resource['id'],\n",
        "                    'target': concept['id'],\n",
        "                    'type': 'explains'\n",
        "                })\n",
        "                explains_edges += 1\n",
        "\n",
        "#exemplifies\n",
        "exemplifies_edges = 0\n",
        "for example in example_nodes:\n",
        "    if example['snippet'] is not None: # Add check for None\n",
        "        example_text = example['snippet'].lower()\n",
        "        for concept in concept_nodes:\n",
        "            if concept['title'].lower() in example_text:\n",
        "                edges.append({\n",
        "                    'source': example['id'],\n",
        "                    'target': concept['id'],\n",
        "                    'type': 'exemplifies'\n",
        "                })\n",
        "                exemplifies_edges += 1\n",
        "\n",
        "#near transfers\n",
        "near_transfer_edges = 0\n",
        "concept_titles = [c['title'] for c in concept_nodes]\n",
        "batch_size = 4\n",
        "\n",
        "for i in range(0, len(concept_titles), batch_size):\n",
        "    batch = concept_titles[i:i + batch_size]\n",
        "\n",
        "    if len(batch) < 2:\n",
        "        continue\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze relationships between these concepts: {batch}\n",
        "\n",
        "    For each PAIR with a clear relationship, return JSON array:\n",
        "    [\n",
        "      {{\n",
        "        \"source\": \"Concept1\",\n",
        "        \"target\": \"Concept2\",\n",
        "        \"subtype\": \"siblings\" or \"contrasts_with\" or \"is_a\" or \"part_of\" or \"uses_in\"\n",
        "      }}\n",
        "    ]\n",
        "\n",
        "    Rules:\n",
        "    - siblings: closely related, same parent topic\n",
        "    - contrasts_with: similar but different/opposing\n",
        "    - is_a: one is type/subclass of another\n",
        "    - part_of: one is component of another\n",
        "    - uses_in: one uses/employs the other\n",
        "\n",
        "    Only include relationships you're confident about.\n",
        "    \"\"\"\n",
        "\n",
        "    relationships = ask_llm(prompt, json_format=\"list\")\n",
        "\n",
        "    if not relationships:\n",
        "        continue\n",
        "\n",
        "    for rel in relationships:\n",
        "        source_title = rel.get('source', '').strip()\n",
        "        target_title = rel.get('target', '').strip()\n",
        "        subtype = rel.get('subtype', 'siblings')\n",
        "\n",
        "        # Find the actual concept nodes\n",
        "        source_concept = next((c for c in concept_nodes if c['title'] == source_title), None)\n",
        "        target_concept = next((c for c in concept_nodes if c['title'] == target_title), None)\n",
        "\n",
        "        if source_concept and target_concept and source_concept['id'] != target_concept['id']:\n",
        "            # Add bidirectional near_transfer\n",
        "            edges.append({\n",
        "                'source': source_concept['id'],\n",
        "                'target': target_concept['id'],\n",
        "                'type': 'near_transfer',\n",
        "                'subtype': subtype\n",
        "            })\n",
        "            edges.append({\n",
        "                'source': target_concept['id'],\n",
        "                'target': source_concept['id'],\n",
        "                'type': 'near_transfer',\n",
        "                'subtype': subtype\n",
        "            })\n",
        "            near_transfer_edges += 2\n",
        "\n",
        "#prerequisites\n",
        "if concept_titles:\n",
        "    prompt = f\"\"\"\n",
        "    Analyze these educational concepts for PREREQUISITE relationships:\n",
        "    {concept_titles}\n",
        "\n",
        "    Return JSON array of prerequisite pairs (A must be learned before B):\n",
        "    [\n",
        "      {{\"source\": \"PrerequisiteConcept\", \"target\": \"DependentConcept\"}}\n",
        "    ]\n",
        "\n",
        "    Example: Algebra should be learned before Calculus → {{\"source\": \"Algebra\", \"target\": \"Calculus\"}}\n",
        "    Only include clear prerequisite relationships.\n",
        "    \"\"\"\n",
        "\n",
        "    prerequisites = ask_llm(prompt, json_format=\"list\")\n",
        "    prereq_edges = 0\n",
        "\n",
        "    if prerequisites:\n",
        "        for prereq in prerequisites:\n",
        "            source_title = prereq.get('source', '').strip()\n",
        "            target_title = prereq.get('target', '').strip()\n",
        "\n",
        "            source_concept = next((c for c in concept_nodes if c['title'] == source_title), None)\n",
        "            target_concept = next((c for c in concept_nodes if c['title'] == target_title), None)\n",
        "\n",
        "            if source_concept and target_concept and source_concept['id'] != target_concept['id']:\n",
        "                edges.append({\n",
        "                    'source': source_concept['id'],\n",
        "                    'target': target_concept['id'],\n",
        "                    'type': 'prereq_of'\n",
        "                })\n",
        "                prereq_edges += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlnz8gPKvyi_",
        "outputId": "f1de7dfe-877e-4df0-96fb-e97fc2f5a9cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concepts: 15\n",
            "Resources: 103\n",
            "Examples: 0\n",
            "Total Edges: 125\n",
            "   explains: 109 edges\n",
            "   near_transfer: 14 edges (7 unique pairs)\n",
            "   prereq_of: 2 edges\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "knowledge_graph = {\n",
        "    'concepts': concept_nodes,\n",
        "    'resources': resource_nodes,\n",
        "    'examples': example_nodes,\n",
        "    'edges': edges\n",
        "}\n",
        "\n",
        "print(f\"Concepts: {len(concept_nodes)}\")\n",
        "print(f\"Resources: {len(resource_nodes)}\")\n",
        "print(f\"Examples: {len(example_nodes)}\")\n",
        "print(f\"Total Edges: {len(edges)}\")\n",
        "\n",
        "edge_types = Counter([e['type'] for e in edges])\n",
        "for edge_type, count in edge_types.items():\n",
        "    if edge_type == 'near_transfer':\n",
        "        # Count unique near_transfer pairs (divide by 2 since bidirectional)\n",
        "        unique_pairs = count // 2\n",
        "        print(f\"   {edge_type}: {count} edges ({unique_pairs} unique pairs)\")\n",
        "    else:\n",
        "        print(f\"   {edge_type}: {count} edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMaW2rAlv5TN"
      },
      "outputs": [],
      "source": [
        "#Get concepts from query\n",
        "def extract_concepts_from_query(query, knowledge_graph, llm_api):\n",
        "    \"\"\"\n",
        "    Use LLM to identify which concept(s) in graph the query is about\n",
        "    \"\"\"\n",
        "    all_concept_titles = [c['title'] for c in knowledge_graph['concepts']]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    USER QUERY: \"{query}\"\n",
        "\n",
        "    AVAILABLE CONCEPTS IN KNOWLEDGE BASE:\n",
        "    {all_concept_titles}\n",
        "\n",
        "    TASK: Which concept(s) from the list is the user asking about?\n",
        "\n",
        "    Return JSON array of concept titles:\n",
        "    [\"Concept Title 1\", \"Concept Title 2\", ...]\n",
        "\n",
        "    Rules:\n",
        "    1. Only include concepts from the AVAILABLE CONCEPTS list\n",
        "    2. If query is about multiple concepts, include all relevant ones\n",
        "    3. If unsure, pick the most likely one\n",
        "    4. Return empty array [] if no match\n",
        "\n",
        "    Example query: \"How do neural networks work?\"\n",
        "    Response: [\"Neural Networks\"]\n",
        "\n",
        "    Example query: \"Explain calculus and algebra\"\n",
        "    Response: [\"Calculus\", \"Algebra\"]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm_api(prompt)\n",
        "        # Parse JSON response\n",
        "        import json\n",
        "        if '```json' in response:\n",
        "            json_str = response.split('```json')[1].split('```')[0]\n",
        "        elif '```' in response:\n",
        "            json_str = response.split('```')[1].split('```')[0]\n",
        "        else:\n",
        "            json_str = response.strip()\n",
        "\n",
        "        concept_titles = json.loads(json_str)\n",
        "\n",
        "        # Verify these are actually in our graph\n",
        "        valid_concepts = []\n",
        "        for title in concept_titles:\n",
        "            for concept in knowledge_graph['concepts']:\n",
        "                if concept['title'] == title:\n",
        "                    valid_concepts.append(concept)\n",
        "                    break\n",
        "\n",
        "        return valid_concepts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting concepts: {e}\")\n",
        "\n",
        "        # Fallback: simple keyword matching\n",
        "        query_lower = query.lower()\n",
        "        matched_concepts = []\n",
        "        for concept in knowledge_graph['concepts']:\n",
        "            if concept['title'].lower() in query_lower:\n",
        "                matched_concepts.append(concept)\n",
        "\n",
        "        return matched_concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pFQv-riGv8D6"
      },
      "outputs": [],
      "source": [
        "#Create subgraph\n",
        "def create_subgraph(target_concept_name, main_graph):\n",
        "    \"\"\"Create teaching subgraph for a concept\"\"\"\n",
        "\n",
        "    # STEP 1: FIND the target node\n",
        "    target_node = None\n",
        "    for concept in main_graph['concepts']:\n",
        "        if concept['title'] == target_concept_name:\n",
        "            target_node = concept\n",
        "            break\n",
        "\n",
        "    if not target_node:\n",
        "        return None\n",
        "\n",
        "    # STEP 2: INITIALIZE empty subgraph\n",
        "    subgraph = {\n",
        "        'concepts': [target_node],  # Start with target\n",
        "        'resources': [],\n",
        "        'examples': [],\n",
        "        'edges': []\n",
        "    }\n",
        "\n",
        "    # STEP 3: EXPAND - Get prerequisites\n",
        "    for edge in main_graph['edges']:\n",
        "        if edge['type'] == 'prereq_of' and edge['target'] == target_node['id']:\n",
        "            # Found prerequisite edge: source → target_node\n",
        "            prereq_id = edge['source']\n",
        "\n",
        "            # Find the prerequisite concept\n",
        "            for concept in main_graph['concepts']:\n",
        "                if concept['id'] == prereq_id:\n",
        "                    subgraph['concepts'].append(concept)\n",
        "                    subgraph['edges'].append(edge)\n",
        "                    break\n",
        "\n",
        "    # STEP 4: EXPAND - Get resources for ALL concepts in subgraph\n",
        "    for concept in subgraph['concepts']:\n",
        "        for edge in main_graph['edges']:\n",
        "            if edge['type'] == 'explains' and edge['target'] == concept['id']:\n",
        "                # Found resource that explains this concept\n",
        "                resource_id = edge['source']\n",
        "\n",
        "                for resource in main_graph['resources']:\n",
        "                    if resource['id'] == resource_id:\n",
        "                        subgraph['resources'].append(resource)\n",
        "                        subgraph['edges'].append(edge)\n",
        "                        break\n",
        "\n",
        "    # STEP 5: EXPAND - Get examples\n",
        "    for concept in subgraph['concepts']:\n",
        "        for edge in main_graph['edges']:\n",
        "            if edge['type'] == 'exemplifies' and edge['target'] == concept['id']:\n",
        "                example_id = edge['source']\n",
        "\n",
        "                for example in main_graph['examples']:\n",
        "                    if example['id'] == example_id:\n",
        "                        subgraph['examples'].append(example)\n",
        "                        subgraph['edges'].append(edge)\n",
        "                        break\n",
        "\n",
        "    return subgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wY0DPvmjv-G7"
      },
      "outputs": [],
      "source": [
        "def create_subgraph_for_concepts(target_concepts, knowledge_graph):\n",
        "    \"\"\"\n",
        "    Creates unified subgraph for MULTIPLE target concepts\n",
        "    Uses your existing create_subgraph() function\n",
        "    \"\"\"\n",
        "\n",
        "    if not target_concepts:\n",
        "        return {\n",
        "            'concepts': [], 'resources': [], 'examples': [], 'edges': []\n",
        "        }\n",
        "\n",
        "    # If only one concept, use existing function\n",
        "    if len(target_concepts) == 1:\n",
        "        return create_subgraph(target_concepts[0]['title'], knowledge_graph)\n",
        "\n",
        "    # For multiple concepts: merge subgraphs\n",
        "    all_subgraphs = []\n",
        "\n",
        "    # Create subgraph for EACH target concept\n",
        "    for concept in target_concepts:\n",
        "        subgraph = create_subgraph(concept['title'], knowledge_graph)\n",
        "        if subgraph:\n",
        "            all_subgraphs.append(subgraph)\n",
        "\n",
        "    # Merge all subgraphs, removing duplicates\n",
        "    merged = merge_subgraphs(all_subgraphs)\n",
        "\n",
        "    return merged\n",
        "\n",
        "def merge_subgraphs(subgraph_list):\n",
        "    \"\"\"\n",
        "    Merge multiple subgraphs, removing duplicates\n",
        "    \"\"\"\n",
        "    merged = {\n",
        "        'concepts': [],\n",
        "        'resources': [],\n",
        "        'examples': [],\n",
        "        'edges': []\n",
        "    }\n",
        "\n",
        "    seen_concept_ids = set()\n",
        "    seen_resource_ids = set()\n",
        "    seen_example_ids = set()\n",
        "    seen_edges = set()  # Track edges by (source, target, type)\n",
        "\n",
        "    for subgraph in subgraph_list:\n",
        "        # Merge concepts\n",
        "        for concept in subgraph['concepts']:\n",
        "            if concept['id'] not in seen_concept_ids:\n",
        "                merged['concepts'].append(concept)\n",
        "                seen_concept_ids.add(concept['id'])\n",
        "\n",
        "        # Merge resources\n",
        "        for resource in subgraph['resources']:\n",
        "            if resource['id'] not in seen_resource_ids:\n",
        "                merged['resources'].append(resource)\n",
        "                seen_resource_ids.add(resource['id'])\n",
        "\n",
        "        # Merge examples\n",
        "        for example in subgraph['examples']:\n",
        "            if example['id'] not in seen_example_ids:\n",
        "                merged['examples'].append(example)\n",
        "                seen_example_ids.add(example['id'])\n",
        "\n",
        "        # Merge edges\n",
        "        for edge in subgraph['edges']:\n",
        "            edge_key = (edge['source'], edge['target'], edge['type'])\n",
        "            if edge_key not in seen_edges:\n",
        "                merged['edges'].append(edge)\n",
        "                seen_edges.add(edge_key)\n",
        "\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cEtph5lwAXt"
      },
      "outputs": [],
      "source": [
        "def sort_concepts_by_prerequisites(subgraph):\n",
        "    \"\"\"\n",
        "    Topological sort based on prereq_of edges\n",
        "    Returns concepts in order: prerequisites first\n",
        "    \"\"\"\n",
        "\n",
        "    # Build adjacency list: prereq -> dependent\n",
        "    graph = {}\n",
        "    for concept in subgraph['concepts']:\n",
        "        graph[concept['id']] = []\n",
        "\n",
        "    # Add edges from subgraph\n",
        "    for edge in subgraph['edges']:\n",
        "        if edge['type'] == 'prereq_of':\n",
        "            if edge['source'] in graph and edge['target'] in graph:\n",
        "                graph[edge['source']].append(edge['target'])\n",
        "\n",
        "    #Count incoming edges (prerequisites) for each node\n",
        "    in_degree = {cid: 0 for cid in graph}\n",
        "    for cid in graph:\n",
        "        for neighbor in graph[cid]:\n",
        "            in_degree[neighbor] = in_degree.get(neighbor, 0) + 1\n",
        "\n",
        "    #Start with nodes that have no prerequisites\n",
        "    from collections import deque\n",
        "    queue = deque([cid for cid in graph if in_degree[cid] == 0])\n",
        "    sorted_ids = []\n",
        "\n",
        "    #Process nodes\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        sorted_ids.append(node)\n",
        "\n",
        "        # Reduce in-degree for neighbors\n",
        "        for neighbor in graph[node]:\n",
        "            in_degree[neighbor] -= 1\n",
        "            if in_degree[neighbor] == 0:\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    #Convert back to concept objects\n",
        "    id_to_concept = {c['id']: c for c in subgraph['concepts']}\n",
        "    sorted_concepts = [id_to_concept[cid] for cid in sorted_ids]\n",
        "\n",
        "    return sorted_concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DbyuBFHwA-9"
      },
      "outputs": [],
      "source": [
        "def extract_references_from_subgraph(subgraph):\n",
        "    \"\"\"Extract reference/citation information from subgraph\"\"\"\n",
        "\n",
        "    references = []\n",
        "\n",
        "    #Extract resource references\n",
        "    for resource in subgraph.get('resources', []):\n",
        "        ref = {\n",
        "            'type': 'resource',\n",
        "            'title': resource.get('title', ''),\n",
        "            'url': resource.get('url', ''),\n",
        "            'format': resource.get('type', 'web')  # pdf, video, web, etc.\n",
        "        }\n",
        "        if resource.get('timecodes'):\n",
        "            ref['timecodes'] = resource['timecodes']\n",
        "        references.append(ref)\n",
        "\n",
        "    #Extract example references\n",
        "    for example in subgraph.get('examples', []):\n",
        "        ref = {\n",
        "            'type': 'example',\n",
        "            'title': example.get('title', ''),\n",
        "            'snippet_preview': example.get('snippet', '')[:100] + '...'\n",
        "        }\n",
        "        references.append(ref)\n",
        "\n",
        "    #Format for display\n",
        "    formatted = []\n",
        "    for ref in references:\n",
        "        if ref['type'] == 'resource':\n",
        "            line = f\"- {ref['title']}\"\n",
        "            if ref.get('url'):\n",
        "                line += f\" ({ref['url']})\"\n",
        "            if ref.get('timecodes'):\n",
        "                line += f\" [Timecodes: {ref['timecodes']}]\"\n",
        "            formatted.append(line)\n",
        "        elif ref['type'] == 'example':\n",
        "            formatted.append(f\"- Example: {ref['title']} - {ref['snippet_preview']}\")\n",
        "\n",
        "    return \"\\n\".join(formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VJRavSKAwDdB"
      },
      "outputs": [],
      "source": [
        "def format_subgraph_for_llm(subgraph, query):\n",
        "    \"\"\"Convert subgraph into LLM-friendly teaching format\"\"\"\n",
        "\n",
        "    # Sort concepts by prerequisites (simple first → complex)\n",
        "    sorted_concepts = sort_concepts_by_prerequisites(subgraph)\n",
        "\n",
        "    formatted = f\"\"\"\n",
        "    QUERY: {query}\n",
        "\n",
        "    TEACHING MATERIALS:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    for i, concept in enumerate(sorted_concepts):\n",
        "        formatted += f\"\"\"\n",
        "    CONCEPT {i+1}: {concept['title']}\n",
        "    Difficulty: {concept.get('difficulty', 'intermediate')}\n",
        "    Definition: {concept.get('definitions', [''])[0]}\n",
        "\n",
        "    Resources:\n",
        "    \"\"\"\n",
        "        # Add resources for this concept\n",
        "        for resource in extract_references_from_subgraph(subgraph):\n",
        "            formatted += f\"    - {resource['title']}\"\n",
        "            if resource.get('type') == 'video' and resource.get('timecodes'):\n",
        "                formatted += f\" [Video: {resource['timecodes'][0]}]\"\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        # Add examples for this concept\n",
        "        examples = extract_references_from_subgraph(subgraph)\n",
        "        if examples:\n",
        "            formatted += \"    Examples:\\n\"\n",
        "            for example in examples:\n",
        "                formatted += f\"    - {example['snippet'][:100]}...\\n\"\n",
        "\n",
        "        formatted += \"\\n\" + \"-\"*50 + \"\\n\"\n",
        "\n",
        "    return formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3AgC_Ov-wFZB"
      },
      "outputs": [],
      "source": [
        "def generate_answer_from_subgraph(subgraph, query, llm_api):\n",
        "    \"\"\"Use LLM to create teaching explanation from subgraph\"\"\"\n",
        "\n",
        "    # Format the subgraph context\n",
        "    context = format_subgraph_for_llm(subgraph, query)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert tutor. Using the provided teaching materials,\n",
        "    answer this query: \"{query}\"\n",
        "\n",
        "    {context}\n",
        "\n",
        "    INSTRUCTIONS FOR YOUR ANSWER:\n",
        "    1. Start with simplest/foundational concepts first\n",
        "    2. Build up to complex concepts using prerequisites\n",
        "    3. Reference specific resources when explaining key points\n",
        "    4. Include relevant examples when helpful\n",
        "    5. Use clear section headings\n",
        "    6. End with a summary\n",
        "\n",
        "    FORMAT:\n",
        "    - Use ### for section headings\n",
        "    - Use **bold** for key terms\n",
        "    - Reference materials like: [See: Resource Title]\n",
        "    - Put code examples in ``` blocks\n",
        "\n",
        "    Generate a comprehensive teaching explanation:\n",
        "    \"\"\"\n",
        "\n",
        "    answer = llm_api(prompt)\n",
        "\n",
        "    # Add citations/references section\n",
        "    references = extract_references_from_subgraph(subgraph)\n",
        "    final_answer = answer + references\n",
        "\n",
        "    return final_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBejB6M7wHai"
      },
      "outputs": [],
      "source": [
        "def answer_query(query, knowledge_graph, llm_api):\n",
        "    \"\"\"Complete: Query → Answer using YOUR functions\"\"\"\n",
        "\n",
        "    #Extract target concept(s) from query\n",
        "    target_concepts = extract_concepts_from_query(query, knowledge_graph, llm_api)\n",
        "\n",
        "    if not target_concepts:\n",
        "        return \"I couldn't identify what you're asking about.\"\n",
        "\n",
        "    #Create subgraph (handles single OR multiple concepts)\n",
        "    subgraph = create_subgraph_for_concepts(target_concepts, knowledge_graph)\n",
        "\n",
        "    if not subgraph['concepts']:\n",
        "        return f\"I don't have information about those concepts.\"\n",
        "\n",
        "    #Sort concepts for teaching\n",
        "    sorted_concepts = sort_concepts_by_prerequisites(subgraph)\n",
        "\n",
        "    formatted_context = format_subgraph_for_llm(subgraph, query)\n",
        "\n",
        "    #Generate answer\n",
        "    answer = llm_api(f\"\"\"You are a tutor. Answer this query: {query}\n",
        "\n",
        "Teaching materials in order:\n",
        "{formatted_context}\n",
        "\n",
        "Generate a clear explanation that starts with prerequisites.\"\"\")\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmXn-PN1wJLG",
        "outputId": "a62c5732-88ff-4233-c152-0088273499e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#vector database\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance,VectorParams, PointStruct\n",
        "\n",
        "# set up database\n",
        "client = QdrantClient(\"localhost\", port=6333)\n",
        "client.create_collection(\n",
        "    collection_name=\"knowledge_graph\",\n",
        "    vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "#insertion of embedding vectors\n",
        "\n",
        "def get_node_text_for_embedding(node):\n",
        "    if node.get('type') == 'web':\n",
        "        return node.get('text_snippet', '')\n",
        "    elif node.get('type') == 'concept':\n",
        "        return node.get('title', '') + \" \" + \". \".join(node.get('definitions', []))\n",
        "    # Add other node types if they exist, or default\n",
        "    return node.get('title', '') # Fallback to title if no other text is found\n",
        "\n",
        "node_texts = [get_node_text_for_embedding(node) for node in all_nodes]\n",
        "embeddings = model.encode(node_texts)\n",
        "\n",
        "points = []\n",
        "for i, (node, embedding) in enumerate(zip(all_nodes, embeddings)):\n",
        "    # Determine the text to store in the payload based on node type\n",
        "    payload_text = \"\"\n",
        "    if node.get('type') == 'web':\n",
        "        payload_text = node.get('text_snippet', '')\n",
        "    elif node.get('type') == 'concept':\n",
        "        payload_text = node.get('title', '') + \" \" + \". \".join(node.get('definitions', []))\n",
        "    else:\n",
        "        payload_text = node.get('title', '')\n",
        "\n",
        "    point = PointStruct(\n",
        "        id=node[\"id\"],\n",
        "        vector=embedding.tolist(),\n",
        "        payload={\n",
        "            \"type\": node.get(\"type\", \"concept\"), \n",
        "            \"title\": node.get(\"title\"),\n",
        "            \"text\": payload_text\n",
        "        }\n",
        "    )\n",
        "    points.append(point)\n",
        "\n",
        "client.upsert(\n",
        "    collection_name=\"knowledge_graph\",\n",
        "    points= points\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foFcdhQL1pGa"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.http.models import ScoredPoint\n",
        "from typing import List\n",
        "\n",
        "def similarity_search(query, top_k = 5):\n",
        "  # Embed the query text\n",
        "  query_embedding = model.encode(query).tolist()\n",
        "\n",
        "  raw_results = client.query_points(\n",
        "        collection_name=\"knowledge_graph\",\n",
        "        query=query_embedding,\n",
        "        limit= top_k,\n",
        "        with_payload=True \n",
        "    )\n",
        "  print(f\"DEBUG (similarity_search): Raw results type: {type(raw_results)}\")\n",
        "  print(f\"DEBUG (similarity_search): Raw results value: {raw_results}\")\n",
        "  return raw_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z460AUDlQRFk"
      },
      "outputs": [],
      "source": [
        "def get_knowledge_graph_from_qdrant():\n",
        "    \"\"\"Extract all data from Qdrant into your graph format\"\"\"\n",
        "    # Get all points\n",
        "    scroll_result = client.scroll(\n",
        "        collection_name=\"knowledge_graph\",\n",
        "        limit=1000,\n",
        "        with_payload=True,\n",
        "        with_vectors=False\n",
        "    )\n",
        "\n",
        "    knowledge_graph = {\n",
        "        'concepts': [],\n",
        "        'resources': [],\n",
        "        'examples': [],\n",
        "        'edges': []  \n",
        "    }\n",
        "\n",
        "    for point in scroll_result[0]:\n",
        "        payload = point.payload\n",
        "        node_type = payload.get('type', 'concept')\n",
        "\n",
        "        node_data = {\n",
        "            'id': point.id,\n",
        "            'title': payload.get('title', ''),\n",
        "            'type': node_type\n",
        "        }\n",
        "\n",
        "        # Add text/definitions based on type\n",
        "        if 'text' in payload:\n",
        "            node_data['definitions'] = [payload['text']]\n",
        "\n",
        "        # Add to appropriate list\n",
        "        if node_type == 'concept':\n",
        "            knowledge_graph['concepts'].append(node_data)\n",
        "        elif node_type in ['web', 'pdf', 'video']:\n",
        "            knowledge_graph['resources'].append(node_data)\n",
        "        elif node_type == 'example':\n",
        "            knowledge_graph['examples'].append(node_data)\n",
        "\n",
        "    return knowledge_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "\n",
        "for node in all_nodes:\n",
        "    label = node.get('title', f\"Node {node['id'][:8]}\")\n",
        "    G.add_node(node['id'], label=label)\n",
        "\n",
        "for edge in edges:\n",
        "    G.add_edge(edge['source'], edge['target'])\n",
        "plt.figure(figsize=(12, 10))\n",
        "pos = nx.spring_layout(G, k=2, seed=42)\n",
        "\n",
        "# Draw nodes and edges\n",
        "nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=300, alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5)\n",
        "\n",
        "# Draw labels\n",
        "labels = nx.get_node_attributes(G, 'label')\n",
        "nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
        "\n",
        "plt.title(f\"Knowledge Graph\")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_BBXolg0Qvi5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from qdrant_client.http.models import ScoredPoint\n",
        "from typing import List\n",
        "\n",
        "def integrated_retrieval(query, top_k = 5):\n",
        "  knowledge_graph = get_knowledge_graph_from_qdrant()\n",
        "  raw_search_results = similarity_search(query, top_k = top_k)\n",
        "\n",
        "  print(f\"DEBUG (integrated_retrieval): raw_search_results type: {type(raw_search_results)}\")\n",
        "  print(f\"DEBUG (integrated_retrieval): raw_search_results value: {raw_search_results}\")\n",
        "\n",
        "  # Extract the actual list of ScoredPoint objects from the QueryResponse\n",
        "  if hasattr(raw_search_results, 'points') and isinstance(raw_search_results.points, list):\n",
        "      search_results = raw_search_results.points\n",
        "  else:\n",
        "      print(f\"ERROR: similarity_search did not return expected QueryResponse with 'points' attribute, or 'points' is not a list. Received: {type(raw_search_results)}\")\n",
        "      search_results = [] # Fallback to empty list\n",
        "\n",
        "  target_concepts = []\n",
        "  for result_item in search_results: # Each result_item should now be a ScoredPoint\n",
        "    print(f\"DEBUG (integrated_retrieval) in loop: Type of result_item: {type(result_item)}, Value: {result_item}\")\n",
        "    scored_point = result_item\n",
        "\n",
        "    payload = scored_point.payload\n",
        "    if payload.get('type') == 'concept':\n",
        "      target_concepts.append({\n",
        "          'id': scored_point.id,\n",
        "          'title': payload.get('title'),\n",
        "          'definitions': [payload.get('text', '')[:200] + \"...\" if payload.get('text') else ''],\n",
        "          'score': scored_point.score\n",
        "      })\n",
        "\n",
        "  if not target_concepts:\n",
        "    answer = f\"**Query:** {query}\\n\\n\"\n",
        "    answer += \"**Search Results:**\\n\"\n",
        "    for i, result_item in enumerate(search_results):\n",
        "        print(f\"DEBUG (integrated_retrieval) in fallback loop: Type of result_item: {type(result_item)}, Value: {result_item}\")\n",
        "        scored_point = result_item\n",
        "        title = scored_point.payload.get('title', 'No title')\n",
        "        text = scored_point.payload.get('text', '')[:150]\n",
        "        answer += f\"{i+1}. **{title}** (score: {scored_point.score:.3f})\\n\"\n",
        "        answer += f\"   {text}...\\n\\n\"\n",
        "    return answer\n",
        "\n",
        "  try:\n",
        "    subgraph = create_subgraph_for_concepts(target_concepts, knowledge_graph)\n",
        "    sorted_concepts = sort_concepts_by_prerequisites(subgraph)\n",
        "    answer = generate_answer_from_subgraph(subgraph, query, ollama_llm)\n",
        "\n",
        "    return answer\n",
        "\n",
        "  except Exception as e: # Catch specific exception if possible\n",
        "    print(f\"Subgraph error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    fallback = f\"**Query:** {query}\\n\\n\"\n",
        "    fallback += \"**Relevant Concepts Found:**\\n\"\n",
        "    for concept in target_concepts:\n",
        "      fallback += f\"- **{concept['title']}** (relevance: {concept.get('score', 0):.3f})\\n\"\n",
        "      if concept.get('definitions'):\n",
        "        fallback += f\"  {concept['definitions'][0][:100]}...\\n\"\n",
        "\n",
        "    fallback += \"\\n*(Subgraph construction failed)*\"\n",
        "    return fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXGYkKJowSoq",
        "outputId": "de9404e5-3163-4b5e-ca2c-1c23bf4a457f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kENRtutPMbHU"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_knowledge_graph(message: str, history):\n",
        "  try:\n",
        "    response = integrated_retrieval(message, top_k = 3)\n",
        "    return response\n",
        "  except Exception as e:\n",
        "    error_msg = f\"Error: {str(e)}\"\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    return error_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "reVpRon6VTEL",
        "outputId": "c044b723-4776-480e-f5a5-b06d28f97650"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3973412187.py:1: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-3973412187.py:1: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-3973412187.py:16: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://edd1305e9d024b6f23.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://edd1305e9d024b6f23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with gr.Blocks(\n",
        "    title=\"Erica - Your AI Tutor\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\"\"\"\n",
        "    .gradio-container { max-width: 900px; margin: auto; }\n",
        "    .chatbot { min-height: 500px; }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Erica - Your AI Tutor\n",
        "    Ask questions about AI/ML concepts. I will answer based on your knowledge base.\n",
        "    \"\"\")\n",
        "\n",
        "    # Chat interface\n",
        "    chatbot = gr.Chatbot(\n",
        "        type=\"messages\",\n",
        "        label=\"Conversation\",\n",
        "        height=500\n",
        "    )\n",
        "\n",
        "    # Input area\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"Ask about any AI Topics\",\n",
        "            scale=4,\n",
        "            autofocus=True,\n",
        "            lines=2\n",
        "        )\n",
        "        submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # Controls\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
        "        mode_info = gr.Markdown(\"*Mode: Subgraph Retrieval*\")\n",
        "\n",
        "    # Examples\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            \"What is attention mechanism?\",\n",
        "            \"Explain Variational Autoencoder (VAE)\",\n",
        "            \"How does CLIP work?\",\n",
        "            \"What are positional embeddings?\",\n",
        "            \"Explain self-attention with an example\"\n",
        "        ],\n",
        "        inputs=msg,\n",
        "        label=\"Try these questions:\"\n",
        "    )\n",
        "\n",
        "    # Response handler\n",
        "    def respond(message, chat_history):\n",
        "        if not message.strip():\n",
        "            return \"\", chat_history\n",
        "\n",
        "        bot_response = chat_with_knowledge_graph(message, chat_history)\n",
        "\n",
        "        # Add to history (messages format)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # Connect events\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    submit_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear_btn.click(lambda: [], None, chatbot)\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
